# -*- coding: utf-8 -*-
"""
ä¸€ä½“åŒ–åˆ†é’Ÿæ•°æ®æ›´æ–°å™¨ï¼ˆyfinance, 7dçª—å£ï¼‰
åŠŸèƒ½ï¼š
- ä¸‹è½½æŒ‡å®šsymbolsçš„æœ€è¿‘ N å¤©åˆ†é’Ÿæ•°æ®ï¼ˆé»˜è®¤1m/7dï¼‰
- ä¿å­˜å½“æ—¥â€œåŸå§‹å¿«ç…§â€ï¼šdata/intraday_raw/{SYMBOL}/{YYYYMMDD}.parquet
- åˆå¹¶å…¥â€œç´¯è®¡å†å²åº“â€ï¼šdata/intraday_merged/{SYMBOL}.parquetï¼ˆæŒ‰æ—¶é—´å»é‡ã€æ’åºï¼‰
- æ”¯æŒä»…ä¿ç•™å¸¸è§„äº¤æ˜“æ—¶æ®µ(09:30-16:00 America/New_York)
- å¯é€‰æ¸…ç†æ—©äºNå¤©çš„rawå¿«ç…§
- è¾“å‡ºæ—¥å¿—åˆ° logs/update_YYYYMMDD.log

ç”¨æ³•ç¤ºä¾‹ï¼š
    python update_intraday_allinone.py --symbols AAPL,MSFT,SPY --rth true
    python update_intraday_allinone.py --symbols-file symbols.txt --interval 1m --period 7d
    python update_intraday_allinone.py --interval 5m --period 60d --raw-keep-days 30
"""

import argparse
from datetime import date, datetime, timedelta
from pathlib import Path
import sys
import time
import traceback

import pandas as pd
import yfinance as yf

# ---------------- ç›®å½•ä¸é»˜è®¤å‚æ•° ----------------
RAW_DIR = Path("data/intraday_raw")
MERGED_DIR = Path("data/intraday_merged")
LOG_DIR = Path("logs")

DEFAULT_INTERVAL = "1m"     # 1m | 2m | 5m | 15m | 30m | 60m | 90m
DEFAULT_PERIOD = "7d"       # 1m æœ€å¤§7dï¼›>=2m å¯åˆ°60d
DEFAULT_RTH = False         # ä»…ä¿ç•™å¸¸è§„äº¤æ˜“æ—¶æ®µ(09:30-16:00 America/New_York)
DEFAULT_SLEEP = 0.3         # æ¯åªä¹‹é—´çš„é—´éš”ï¼Œé¿å…é™æµ
DEFAULT_RETRY = 2           # ä¸‹è½½å¤±è´¥é‡è¯•æ¬¡æ•°
DEFAULT_RAW_KEEP_DAYS = 0   # >0æ—¶æ¸…ç†æ—©äºNå¤©çš„åŸå§‹å¿«ç…§

for d in [RAW_DIR, MERGED_DIR, LOG_DIR]:
    d.mkdir(parents=True, exist_ok=True)


# ---------------- å°å·¥å…· ----------------
def log(msg: str, fp=None):
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts}] {msg}"
    print(line)
    if fp:
        fp.write(line + "\n")
        fp.flush()

def save_df(df: pd.DataFrame, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    try:
        df.to_parquet(path)
    except Exception:
        df.to_csv(path.with_suffix(".csv"))
        # ä¸æ‰“å°ä¹Ÿè¡Œï¼Œè¿™é‡Œæé†’ä¸€ä¸‹
        print(f"âš ï¸ æœªå®‰è£… pyarrow æˆ–å†™ parquet å¤±è´¥ï¼Œå·²å›é€€ CSV: {path.with_suffix('.csv').name}")

def load_df(path: Path):
    if not path.exists():
        # å°è¯•csv
        csv = path.with_suffix(".csv")
        if csv.exists():
            try:
                return pd.read_csv(csv, index_col=0, parse_dates=True)
            except Exception:
                return None
        return None
    # å…ˆå°è¯•parquet
    try:
        return pd.read_parquet(path)
    except Exception:
        # å›é€€csv
        csv = path.with_suffix(".csv")
        if csv.exists():
            try:
                return pd.read_csv(csv, index_col=0, parse_dates=True)
            except Exception:
                return None
        return None

def normalize_tz(df: pd.DataFrame, rth_only: bool) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    try:
        if df.index.tz is None:
            df.index = df.index.tz_localize("UTC").tz_convert("America/New_York")
        else:
            df.index = df.index.tz_convert("America/New_York")
    except Exception:
        pass
    # å»é‡ + æ’åº
    df = df[~df.index.duplicated(keep="last")].sort_index()
    # ä»…RTH
    if rth_only:
        try:
            df = df.between_time("09:30", "16:00")
        except Exception:
            pass
    return df

def download_intraday(symbol: str, interval: str, period: str, rth_only: bool,
                      retries: int, sleep_s: float, log_fp=None) -> pd.DataFrame:
    # ä¿æŠ¤ï¼š1mæ—¶å¼ºåˆ¶period=7d
    if interval == "1m" and period != "7d":
        log(f"âš ï¸ {symbol}: 1m ä»…æ”¯æŒ 7dï¼Œè‡ªåŠ¨æ”¹ä¸º 7d", log_fp)
        period = "7d"

    last_err = None
    for attempt in range(1, retries + 2):
        try:
            log(f"â¬‡ï¸ ä¸‹è½½ {symbol} ({interval},{period}) å°è¯• {attempt}", log_fp)
            df = yf.download(symbol, period=period, interval=interval,
                             progress=False, auto_adjust=False)
            if df is None or df.empty:
                raise RuntimeError("Empty dataframe")
            df = normalize_tz(df, rth_only)
            return df
        except Exception as e:
            last_err = e
            log(f"âŒ {symbol} ä¸‹è½½å¤±è´¥: {e}", log_fp)
            if attempt <= retries:
                time.sleep(min(2.0, sleep_s + 0.5))
            else:
                break
    log(f"âš ï¸ {symbol}: å¤šæ¬¡å°è¯•ä»å¤±è´¥ï¼Œè·³è¿‡ã€‚æœ€åé”™è¯¯ï¼š{last_err}", log_fp)
    return pd.DataFrame()

def snapshot_save(symbol: str, df: pd.DataFrame, log_fp=None):
    if df is None or df.empty:
        return None
    day_str = date.today().strftime("%Y%m%d")
    snap_dir = RAW_DIR / symbol
    snap_dir.mkdir(parents=True, exist_ok=True)
    snap_path = snap_dir / f"{day_str}.parquet"
    save_df(df, snap_path)
    log(f"ğŸ“ ä¿å­˜åŸå§‹å¿«ç…§: {snap_path}", log_fp)
    return snap_path

def append_to_merged(symbol: str, new_df: pd.DataFrame, log_fp=None):
    merged_path = MERGED_DIR / f"{symbol}.parquet"
    old_df = load_df(merged_path)

    before_rows = 0 if old_df is None or old_df.empty else old_df.shape[0]
    if old_df is None or old_df.empty:
        merged = new_df.copy()
    else:
        merged = pd.concat([old_df, new_df])
        merged = merged[~merged.index.duplicated(keep="last")].sort_index()

    after_rows = 0 if merged is None or merged.empty else merged.shape[0]
    if after_rows > 0:
        save_df(merged, merged_path)
    log(f"ğŸ“¦ åˆå¹¶ {symbol}: æ–°å¢ {max(0, after_rows - before_rows)} è¡Œï¼Œç´¯è®¡ {after_rows} è¡Œ", log_fp)
    return (max(0, after_rows - before_rows), after_rows)

def clean_old_raw(symbols, keep_days: int, log_fp=None):
    if keep_days <= 0:
        return
    cutoff = date.today() - timedelta(days=keep_days)
    removed = 0
    for sym in symbols:
        d = RAW_DIR / sym
        if not d.exists():
            continue
        for p in d.glob("*.parquet"):
            try:
                day = p.stem  # YYYYMMDD
                dt = datetime.strptime(day, "%Y%m%d").date()
                if dt < cutoff:
                    p.unlink(missing_ok=True)
                    # åŒåcsvä¹Ÿåˆ ä¸€ä¸‹
                    csv = p.with_suffix(".csv")
                    if csv.exists():
                        csv.unlink(missing_ok=True)
                    removed += 1
            except Exception:
                continue
    log(f"ğŸ§¹ æ¸…ç†åŸå§‹å¿«ç…§å®Œæˆï¼šåˆ é™¤ {removed} ä¸ªæ—§æ–‡ä»¶ï¼ˆæ—©äº {keep_days} å¤©ï¼‰", log_fp)

def load_symbols(args):
    symbols = []
    if args.symbols:
        symbols += [s.strip().upper() for s in args.symbols.split(",") if s.strip()]
    if args.symbols_file:
        txt = Path(args.symbols_file)
        if txt.exists():
            symbols += [line.strip().upper() for line in txt.read_text().splitlines() if line.strip()]
    # å»é‡ä¿åº
    return list(dict.fromkeys(symbols))

def build_argparser():
    p = argparse.ArgumentParser(description="ä¸€ä½“åŒ–åˆ†é’Ÿæ•°æ®æ›´æ–°å™¨ï¼ˆyfinance, 7dçª—å£ï¼Œåˆå¹¶ç´¯è®¡åº“ï¼‰")
    p.add_argument("--symbols", type=str, default="",
                   help="é€—å·åˆ†éš”è‚¡ç¥¨ä»£ç ï¼Œå¦‚ï¼šAAPL,MSFT,SPY")
    p.add_argument("--symbols-file", type=str, default="",
                   help="ä»æ–‡æœ¬æ–‡ä»¶è¯»å–ä»£ç ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰")
    p.add_argument("--interval", type=str, default=DEFAULT_INTERVAL,
                   help="1m/2m/5m/15m/30m/60m/90mã€‚1m æœ€å¤§ä»…æ”¯æŒ 7d")
    p.add_argument("--period", type=str, default=DEFAULT_PERIOD,
                   help="æ—¶é—´çª—å£ã€‚1m=7dï¼›>=2m å¯åˆ° 60d")
    p.add_argument("--rth", type=str, default=str(DEFAULT_RTH).lower(),
                   help="ä»…ä¿ç•™å¸¸è§„äº¤æ˜“æ—¶æ®µï¼ˆ09:30-16:00ï¼‰ï¼Œtrue/false")
    p.add_argument("--sleep", type=float, default=DEFAULT_SLEEP,
                   help="ä¸¤åªè‚¡ç¥¨ä¹‹é—´ä¼‘çœ ç§’æ•°")
    p.add_argument("--retries", type=int, default=DEFAULT_RETRY,
                   help="ä¸‹è½½å¤±è´¥é‡è¯•æ¬¡æ•°")
    p.add_argument("--raw-keep-days", type=int, default=DEFAULT_RAW_KEEP_DAYS,
                   help="æ¸…ç†æ—©äºNå¤©çš„åŸå§‹å¿«ç…§ï¼ˆ0è¡¨ç¤ºä¸æ¸…ç†ï¼‰")
    return p


def main():
    args = build_argparser().parse_args()
    symbols = load_symbols(args)
    if not symbols:
        print("â—è¯·ä½¿ç”¨ --symbols æˆ– --symbols-file æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨")
        sys.exit(1)

    interval = args.interval.strip()
    period = args.period.strip()
    rth_only = args.rth.strip().lower() in ("1","true","yes","y")
    sleep_s = float(args.sleep)
    retries = int(args.retries)
    raw_keep_days = int(args.raw_keep_days)

    # æ—¥å¿—æ–‡ä»¶
    log_path = LOG_DIR / f"update_{date.today().strftime('%Y%m%d')}.log"
    with open(log_path, "a", encoding="utf-8") as log_fp:
        log(f"=== å¯åŠ¨æ›´æ–°ï¼šsymbols={len(symbols)}, interval={interval}, period={period}, RTH={rth_only} ===", log_fp)

        total_success = 0
        total_added_rows = 0
        t0 = time.time()

        for i, sym in enumerate(symbols, 1):
            try:
                log(f"[{i}/{len(symbols)}] å¤„ç† {sym}", log_fp)
                df = download_intraday(sym, interval, period, rth_only, retries, sleep_s, log_fp)
                if df.empty:
                    log(f"âš ï¸ {sym}: æ— æ•°æ®æˆ–ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡åˆå¹¶", log_fp)
                else:
                    snapshot_save(sym, df, log_fp)
                    added, total = append_to_merged(sym, df, log_fp)
                    total_added_rows += added
                    total_success += 1
                time.sleep(sleep_s)
            except Exception as e:
                log(f"âŒ {sym}: æœªå¤„ç†å¼‚å¸¸: {e}", log_fp)
                traceback.print_exc(file=log_fp)
                time.sleep(min(2.0, sleep_s + 0.5))

        if raw_keep_days > 0:
            clean_old_raw(symbols, raw_keep_days, log_fp)

        dt = time.time() - t0
        log(f"âœ… å®Œæˆï¼šæˆåŠŸ {total_success}/{len(symbols)}ï¼Œæ–°å¢è®°å½• {total_added_rows} è¡Œï¼Œç”¨æ—¶ {dt:.1f}s", log_fp)
        log(f"æ—¥å¿—ä½ç½®ï¼š{log_path}", log_fp)


if __name__ == "__main__":
    main()
